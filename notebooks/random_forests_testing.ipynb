{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\coding\\Uni\\MA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\justi\\mambaforge-pypy3\\envs\\MA\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from evoaudio.population import Population\n",
    "from evoaudio.sample_library import SampleLibrary\n",
    "from evoaudio.feature_extraction import extract_features_for_window, extract_features_for_windows\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population to feature vector of shape (n_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The following statistics are proposed as approximative features.  \n",
    "For each instrument and each onset in the approximated\n",
    "music track, we save the smallest distance between the best\n",
    "candidate mixture which contains this instrument and the onset\n",
    "to approximate.  \n",
    "(pop.best_collections_per_onset)  \n",
    "  \n",
    "These smallest distances are kept during the\n",
    "complete evolutionary loop in an archive and do not represent\n",
    "the final population only. (TODO!) \n",
    "    \n",
    "Then, we estimate the mean, the\n",
    "minimum, and the maximum values for each of 51 instrument\n",
    "and 88 theoretically possible pitches for two different analysis\n",
    "frames of 10s and 3s. Additionally, we sort the recognised\n",
    "instruments based on the smallest distances, and assign ranks\n",
    "to corresponding approximative features, e.g., value of “rang\n",
    "of acoustic guitar” = 1 means that acoustic guitar had the\n",
    "smallest mean distance between approximations with this\n",
    "instrument and unknown onsets in the analysis frame. This\n",
    "leads to an overall number of feature dimensions equal to\n",
    "(51 · 3 + 88 · 3 + 51) · 2 = 936.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples: 100%|██████████| 6478/6478 [00:16<00:00, 403.26it/s]\n"
     ]
    }
   ],
   "source": [
    "pop = Population.from_file(\"30k_gen_nutcracker.pkl\")\n",
    "target, sr = librosa.load(librosa.ex('nutcracker'), duration=30)\n",
    "lib = SampleLibrary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467 features generated from 50 instruments and 89 pitches.\n"
     ]
    }
   ],
   "source": [
    "## Choose two different analysis frames of 10s and 3s\n",
    "end_offset = 10 * sr\n",
    "possible_onsets = len(target) - end_offset\n",
    "window_start = np.random.randint(low=0, high=possible_onsets)\n",
    "window_end = window_start + end_offset\n",
    "\n",
    "## Grab the onsets in those windows, and the associated best records from the population\n",
    "relevant_collections = [collection for collection in pop.best_collections_per_onset.values() if collection.onset in range(window_start, window_end)]\n",
    "\n",
    "## For each window separately, calculate the maximum, minimum, and mean fitnesses for each occasion of\n",
    "## An instrument\n",
    "instrument_occurrences_fitness = dict()\n",
    "## A pitch\n",
    "pitch_occurrences_fitness = dict()\n",
    "\n",
    "for collection in relevant_collections:\n",
    "    for sample in collection.samples:\n",
    "        if sample.instrument in instrument_occurrences_fitness:\n",
    "            instrument_occurrences_fitness[sample.instrument].append(collection.fitness)\n",
    "        else:\n",
    "            instrument_occurrences_fitness[sample.instrument] = [collection.fitness]\n",
    "        if sample.pitch in pitch_occurrences_fitness:\n",
    "            pitch_occurrences_fitness[sample.pitch].append(collection.fitness)\n",
    "        else:\n",
    "            pitch_occurrences_fitness[sample.pitch] = [collection.fitness]\n",
    "\n",
    "# print(instrument_occurrences_fitness)\n",
    "# print(pitch_occurrences_fitness)\n",
    "\n",
    "instrument_min = {instrument: np.min(instrument_occurrences_fitness[instrument]) for instrument in instrument_occurrences_fitness}\n",
    "instrument_max = {instrument: np.max(instrument_occurrences_fitness[instrument]) for instrument in instrument_occurrences_fitness}\n",
    "instrument_mean = {instrument: np.mean(instrument_occurrences_fitness[instrument]) for instrument in instrument_occurrences_fitness}\n",
    "\n",
    "pitch_min = {pitch: np.min(pitch_occurrences_fitness[pitch]) for pitch in pitch_occurrences_fitness}\n",
    "pitch_max = {pitch: np.max(pitch_occurrences_fitness[pitch]) for pitch in pitch_occurrences_fitness}\n",
    "pitch_mean = {pitch: np.mean(pitch_occurrences_fitness[pitch]) for pitch in pitch_occurrences_fitness}\n",
    "\n",
    "## Finally, give each instrument a rank from 1 to n_instruments, based on their mean distances (smallest = rank 1, highest = rank n_instruments)\n",
    "# instrument_sort = np.argsort([instrument_mean[instrument] for instrument in instrument_mean])\n",
    "instrument_sort = {k: v for k, v in sorted(instrument_mean.items(), key=lambda item: item[1])}\n",
    "instrument_ranks = {instrument: i + 1 for i, instrument in enumerate(instrument_sort)}\n",
    "\n",
    "# Create feature vector\n",
    "instrument_features = []\n",
    "for instrument_info in lib.instruments:\n",
    "    instr_name = instrument_info.name\n",
    "    if instr_name in instrument_ranks:\n",
    "        instrument_features.append([instrument_min[instr_name], instrument_mean[instr_name], instrument_max[instr_name], instrument_ranks[instr_name]])\n",
    "    else:\n",
    "        instrument_features.append([np.inf, np.inf, np.inf, np.inf])\n",
    "pitch_features = []\n",
    "for pitch in lib.pitches:\n",
    "    if pitch in pitch_min:\n",
    "        pitch_features.append([pitch_min[pitch], pitch_mean[pitch], pitch_max[pitch]])\n",
    "    else:\n",
    "        pitch_features.append([np.inf, np.inf, np.inf])\n",
    "flat_instr_features = np.array(instrument_features).flatten()\n",
    "flat_pitch_features = np.array(pitch_features).flatten()\n",
    "features = np.concatenate((flat_instr_features, flat_pitch_features))\n",
    "print(f\"{len(features)} features generated from {len(lib.instruments)} instruments and {len(lib.pitches)} pitches.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467 features generated from 50 instruments and 89 pitches.\n"
     ]
    }
   ],
   "source": [
    "end_offset = 10 * sr\n",
    "possible_onsets = len(target) - end_offset\n",
    "window_start = np.random.randint(low=0, high=possible_onsets)\n",
    "window_end = window_start + end_offset\n",
    "features = extract_features_for_window(pop, lib, window_start, window_end)\n",
    "print(f\"{len(features)} features generated from {len(lib.instruments)} instruments and {len(lib.pitches)} pitches.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934 features generated from 50 instruments and 89 pitches.\n"
     ]
    }
   ],
   "source": [
    "features = extract_features_for_windows(pop=pop, lib=lib, window_lengths=[3, 10], n_total_samples=len(target), sr=sr)\n",
    "print(f\"{len(features)} features generated from {len(lib.instruments)} instruments and {len(lib.pitches)} pitches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X, y)\n",
    "print(clf.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c']\n"
     ]
    }
   ],
   "source": [
    "X = [[0], [1], [2]]\n",
    "y = [\"a\", \"b\", \"c\"]\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X, y)\n",
    "print(clf.predict([[2]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Random Forests from Saved Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples: 100%|██████████| 6826/6826 [00:11<00:00, 616.54it/s]\n"
     ]
    }
   ],
   "source": [
    "lib = SampleLibrary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 10k generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './experiments/1517_artists/300_1_10000_0.05_5_10_1_20_0.9954_15_1sec\\\\Hip-Hop-Main_Flow-Hip-Hop_Worth_Dying_For_Featuring_Talib_Kweli.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m      9\u001b[0m songnames \u001b[39m=\u001b[39m [file\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m popfiles]\n\u001b[0;32m     10\u001b[0m \u001b[39m# all_soundfiles = glob(\"./audio/1517-Artists/**/*.mp3\", recursive=True)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# soundfiles = []\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# for name in songnames:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m#             break\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# lib = SampleLibrary()\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m pops \u001b[39m=\u001b[39m [Population\u001b[39m.\u001b[39mfrom_file(popfile\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mHip_Hop\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mHip-Hop\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39mfor\u001b[39;00m popfile \u001b[39min\u001b[39;00m popfiles]\n\u001b[0;32m     19\u001b[0m \u001b[39m# songs = [librosa.load(soundfile)[0] for soundfile in soundfiles]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m sr \u001b[39m=\u001b[39m \u001b[39m22050\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m songnames \u001b[39m=\u001b[39m [file\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m popfiles]\n\u001b[0;32m     10\u001b[0m \u001b[39m# all_soundfiles = glob(\"./audio/1517-Artists/**/*.mp3\", recursive=True)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# soundfiles = []\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# for name in songnames:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m#             break\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# lib = SampleLibrary()\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m pops \u001b[39m=\u001b[39m [Population\u001b[39m.\u001b[39;49mfrom_file(popfile\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39;49m\u001b[39mHip_Hop\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mHip-Hop\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39mfor\u001b[39;00m popfile \u001b[39min\u001b[39;00m popfiles]\n\u001b[0;32m     19\u001b[0m \u001b[39m# songs = [librosa.load(soundfile)[0] for soundfile in soundfiles]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m sr \u001b[39m=\u001b[39m \u001b[39m22050\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\justi\\coding\\Uni\\MA\\evoaudio\\population.py:170\u001b[0m, in \u001b[0;36mPopulation.from_file\u001b[1;34m(cls, filename, expand, sample_lib)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39m@classmethod\u001b[39m \n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_file\u001b[39m(\u001b[39mcls\u001b[39m, filename:\u001b[39mstr\u001b[39m, expand:\u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, sample_lib\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Population:\n\u001b[0;32m    155\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads a population from a pickled file.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39m        The loaded population contained in the given file.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    171\u001b[0m         obj \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(fp)\n\u001b[0;32m    172\u001b[0m         \u001b[39mif\u001b[39;00m expand \u001b[39mand\u001b[39;00m sample_lib \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './experiments/1517_artists/300_1_10000_0.05_5_10_1_20_0.9954_15_1sec\\\\Hip-Hop-Main_Flow-Hip-Hop_Worth_Dying_For_Featuring_Talib_Kweli.pkl'"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Load populations, Load Songs\n",
    "all_popfiles = glob(\"./experiments/1517_artists/300_1_10000_0.05_5_10_1_20_0.9954_15_1sec/*.pkl\")\n",
    "popfiles = [file.replace(\"Hip-Hop\", \"Hip_Hop\") for file in all_popfiles if \"500gens\" not in file and \".logger\" not in file]\n",
    "labels = [os.path.basename(file.split(\"-\")[0]) for file in popfiles]\n",
    "songnames = [file.split(\"-\", 1)[1].split(\".\")[0] for file in popfiles]\n",
    "# all_soundfiles = glob(\"./audio/1517-Artists/**/*.mp3\", recursive=True)\n",
    "# soundfiles = []\n",
    "# for name in songnames:\n",
    "#     for file in all_soundfiles:\n",
    "#         if name in file:\n",
    "#             soundfiles.append(file)\n",
    "#             break\n",
    "# lib = SampleLibrary()\n",
    "pops = [Population.from_file(popfile.replace(\"Hip_Hop\", \"Hip-Hop\")) for popfile in popfiles]\n",
    "# songs = [librosa.load(soundfile)[0] for soundfile in soundfiles]\n",
    "sr = 22050\n",
    "\n",
    "# Separate the archive into 10s and 3s chunks\n",
    "\n",
    "# Create feature vectors for pops for windows of 10s and 3s across the song\n",
    "def pop_to_window_vectors(pop:Population, song_length:int, window_length_s:int, lib:SampleLibrary, sr:int=22050):\n",
    "    window_length = sr*window_length_s\n",
    "    window_indices = list(range(0, song_length, window_length))\n",
    "    all_features = [extract_features_for_window(\n",
    "            pop=pop, lib=lib, \n",
    "            window_start=window_idx, \n",
    "            window_end=min(window_idx+window_length, song_length)) \n",
    "        for window_idx in window_indices]\n",
    "    return np.array(all_features)\n",
    "\n",
    "# features_10s = pop_to_window_vectors(pop=pop, song_length=len(song), window_length_s=10, lib=lib, sr=sr)\n",
    "# features_3s = pop_to_window_vectors(pop=pop, song_length=len(song), window_length_s=3, lib=lib, sr=sr)\n",
    "\n",
    "# Divide songs into 4s windows with 2s overlap\n",
    "def get_features_for_time_window(features, feature_window_length_s, window_start_s, window_end_s):\n",
    "    # Calc in which window we start\n",
    "    start_idx = window_start_s // feature_window_length_s\n",
    "    # Calc in which window we end\n",
    "    end_idx = window_end_s // feature_window_length_s\n",
    "    # Get all features in between\n",
    "    window_features = features[start_idx:(end_idx+1)]\n",
    "    # Calc mean between feature vectors\n",
    "    return np.mean(window_features, axis=0)\n",
    "\n",
    "# song_feature_matrix = np.array([np.concatenate([\n",
    "#     get_features_for_time_window(features_10s, 10, i, i+4), \n",
    "#     get_features_for_time_window(features_3s, 3, i, i+4)]) \n",
    "#     for i in range(0, len(song), sr*2)])\n",
    "\n",
    "# # Create train and test sets\n",
    "# X = song_feature_matrix\n",
    "# y = np.repeat(labels[0], len(X))\n",
    "# # Train Random Forests with 100 trees\n",
    "# song_feature_matrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_to_window_vectors(pop:Population, song_length:int, window_length_s:int, lib:SampleLibrary, sr:int=22050):\n",
    "    window_length = sr*window_length_s\n",
    "    window_indices = list(range(0, song_length, window_length))\n",
    "    all_features = [extract_features_for_window(\n",
    "            pop=pop, lib=lib, \n",
    "            window_start=window_idx, \n",
    "            window_end=min(window_idx+window_length, song_length)) \n",
    "        for window_idx in window_indices]\n",
    "    return np.array(all_features)\n",
    "\n",
    "def get_features_for_time_window(features, feature_window_length_s, window_start_s, window_end_s):\n",
    "    # Calc in which window we start\n",
    "    start_idx = window_start_s // feature_window_length_s\n",
    "    # Calc in which window we end\n",
    "    end_idx = window_end_s // feature_window_length_s\n",
    "    # Get all features in between\n",
    "    window_features = features[start_idx:(end_idx+1)]\n",
    "    # Calc mean between feature vectors\n",
    "    return np.mean(window_features, axis=0)\n",
    "\n",
    "def get_xy_for_pop(pop:Population, label:str, song_length:int, sr:int=22050):\n",
    "    features_10s = pop_to_window_vectors(pop=pop, song_length=song_length, window_length_s=10, lib=lib, sr=sr)\n",
    "    features_3s = pop_to_window_vectors(pop=pop, song_length=song_length, window_length_s=3, lib=lib, sr=sr)\n",
    "    song_feature_matrix = np.array([np.concatenate([\n",
    "        get_features_for_time_window(features_10s, 10, i, i+4), \n",
    "        get_features_for_time_window(features_3s, 3, i, i+4)]) \n",
    "        for i in range(0, int(song_length/sr), 2)])\n",
    "    return song_feature_matrix, np.repeat(label, len(song_feature_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Population files and correct song labels\n",
    "# all_popfiles = glob(\"./experiments/1517_artists/300_1_10000_0.05_5_10_1_20_0.9954_15_1sec/*.pkl\")\n",
    "# Initial Populations (0 Generations)\n",
    "all_popfiles = glob(\"./experiments/1517_artists/300_1_0_0.05_5_10_1_20_0.9954_15_1sec/*.pkl\")\n",
    "# For 500 Generations\n",
    "# popfiles = [file.replace(\"\\\\Hip_Hop-\", \"\\\\Hip-Hop-\") for file in all_popfiles if \"500gens\" in file and \".logger\" not in file]\n",
    "# For 10k Generations\n",
    "popfiles = [file.replace(\"\\\\Hip_Hop-\", \"\\\\Hip-Hop-\") for file in all_popfiles if \"500gens\" not in file and \".logger\" not in file]\n",
    "labels = [os.path.basename(file.split(\"-\")[0]) for file in popfiles]\n",
    "songnames = [file.split(\"-\", 1)[1].split(\".\")[0] for file in popfiles]\n",
    "sr = 22050\n",
    "# Split songs into train/test sets so that we can later get errors per genre label\n",
    "# Get songs per label\n",
    "songs_per_label = {label: 0 for label in set(labels)}\n",
    "for i in range(len(popfiles)):\n",
    "    songs_per_label[labels[i]] += 1\n",
    "# Shuffle\n",
    "rnd_idx = list(range(len(popfiles)))\n",
    "np.random.shuffle(rnd_idx)\n",
    "# 80/20 Train/test split\n",
    "popfiles_train = np.take(popfiles, rnd_idx[:int(0.8*len(rnd_idx))])\n",
    "popfiles_test = np.take(popfiles, rnd_idx[int(0.8*len(rnd_idx)):])\n",
    "labels_train = np.take(labels, rnd_idx[:int(0.8*len(rnd_idx))])\n",
    "labels_test = np.take(labels, rnd_idx[int(0.8*len(rnd_idx)):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2453/2453 [02:27<00:00, 16.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build X, y\n",
    "x_train = []\n",
    "y_train = []\n",
    "# Train Set\n",
    "for i, popfile in enumerate(tqdm(popfiles_train)):\n",
    "    pop = Population.from_file(popfile, expand=False)\n",
    "    label = labels_train[i]\n",
    "    song_length = max(pop.archive.keys()) + 1\n",
    "    x, y_labels = get_xy_for_pop(pop=pop, label=label, song_length=song_length, sr=sr)\n",
    "    x_train.append(x)\n",
    "    y_train.append(y_labels)\n",
    "\n",
    "x_train = np.concatenate(x_train)\n",
    "y_train = np.concatenate(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=7, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=7, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=7, random_state=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=0)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [00:36<00:00, 16.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split Test Set by Song, Label\n",
    "x_test_by_song = []\n",
    "y_test_by_song = []\n",
    "for i, popfile in enumerate(tqdm(popfiles_test)):\n",
    "    pop = Population.from_file(popfile, expand=False)\n",
    "    label = labels_test[i]\n",
    "    song_length = max(pop.archive.keys()) + 1\n",
    "    x, y_labels = get_xy_for_pop(pop=pop, label=label, song_length=song_length, sr=sr)\n",
    "    x_test_by_song.append(x)\n",
    "    y_test_by_song.append(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority Voting across the windows of a piece\n",
    "def majority_voting(votes):\n",
    "    counts = {vote: 0 for vote in set(votes)}\n",
    "    for vote in votes:\n",
    "        counts[vote] += 1\n",
    "    return list(counts.keys())[np.argmax(list(counts.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [00:05<00:00, 122.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate errors for entire songs, per label\n",
    "errors_per_label = {label: [] for label in labels}\n",
    "for i, song_x in enumerate(tqdm(x_test_by_song)):\n",
    "    song_y = y_test_by_song[i]\n",
    "    song_label = y_test_by_song[i][0]\n",
    "    votes = clf.predict(song_x)\n",
    "    vote_winner = majority_voting(votes)\n",
    "    if vote_winner == song_label:\n",
    "        errors_per_label[song_label].append(0)\n",
    "    else:\n",
    "        errors_per_label[song_label].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alternative_and_Punk': 0.8,\n",
       " 'Blues': 0.972972972972973,\n",
       " 'Childrenss': 1.0,\n",
       " 'Classical': 0.2777777777777778,\n",
       " 'Comedy_and_Spoken_Word': 1.0,\n",
       " 'Country': 1.0,\n",
       " 'Easy_Listening_and_Vocals': 1.0,\n",
       " 'Electronic_and_Dance': 0.42857142857142855,\n",
       " 'Folk': 0.8888888888888888,\n",
       " 'Hip': 0.9655172413793104,\n",
       " 'Jazz': 0.696969696969697,\n",
       " 'Latin': 1.0,\n",
       " 'New_Age': 0.8275862068965517,\n",
       " 'Reggae': 1.0,\n",
       " 'Religious': 1.0,\n",
       " 'Rock_and_Pop': 0.7142857142857143,\n",
       " 'R_and_B_and_Soul': 0.8846153846153846,\n",
       " 'Soundtracks_and_More': 1.0,\n",
       " 'World': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_errors_per_label = {label: np.mean(errors) for label, errors in errors_per_label.items()}\n",
    "mean_errors_per_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8661676480188277"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_error_majority_votes = np.mean(list(mean_errors_per_label.values()))\n",
    "mean_error_majority_votes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors per window, irrespective of song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8714480201604727"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.concatenate(x_test_by_song)\n",
    "y_test = np.concatenate(y_test_by_song)\n",
    "predictions = clf.predict(x_test)\n",
    "errors = [1*(not predictions[i] == y_test[i]) for i in range(len(predictions))]\n",
    "np.mean(errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e64d2645d10406c485d9857eaab5d37f8c8efad32a47710fb4de5b20809a4099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
